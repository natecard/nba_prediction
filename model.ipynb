{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "# change the display width to see all columns\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "games_df = pd.read_csv('./data/games.csv', low_memory=False)\n",
    "games_details_df = pd.read_csv('./data/games_details.csv', low_memory=False)\n",
    "# players_df = pd.read_csv('./data/players.csv')\n",
    "ranking_df = pd.read_csv('./data/ranking.csv')\n",
    "teams_df = pd.read_csv('./data/teams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the game dataframe and group by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      GAME_DATE_EST   GAME_ID  HOME_TEAM_ID  VISITOR_TEAM_ID  SEASON  TEAM_ID_home  PTS_home  FG_PCT_home  FT_PCT_home  FG3_PCT_home  AST_home  REB_home  TEAM_ID_away  PTS_away  FG_PCT_away  FT_PCT_away  FG3_PCT_away  AST_away  REB_away  HOME_TEAM_WINS\n",
      "19288    2003-10-05  10300001    1610612762       1610612742    2003    1610612762      90.0        0.457        0.735         0.143      23.0      41.0    1610612742      85.0        0.447        0.500         0.250      20.0      38.0               1\n",
      "19287    2003-10-06  10300002    1610612763       1610612749    2003    1610612763     105.0        0.494        0.618         0.267      25.0      48.0    1610612749      94.0        0.427        0.700         0.154      20.0      43.0               1\n",
      "19280    2003-10-07  10300010    1610612764       1610612752    2003    1610612764     104.0        0.506        0.677         0.455      26.0      45.0    1610612752      86.0        0.380        0.852         0.188      19.0      37.0               1\n",
      "19286    2003-10-07  10300009    1610612758       1610612746    2003    1610612758     101.0        0.467        0.871         0.444      19.0      39.0    1610612746      82.0        0.368        0.609         0.364      13.0      50.0               1\n",
      "19285    2003-10-07  10300005    1610612757       1610612745    2003    1610612757     104.0        0.527        0.657         0.429      22.0      33.0    1610612745      80.0        0.470        0.667         0.333      10.0      37.0               1\n",
      "...             ...       ...           ...              ...     ...           ...       ...          ...          ...           ...       ...       ...           ...       ...          ...          ...           ...       ...       ...             ...\n",
      "19093    2003-11-08  20300081    1610612745       1610612753    2003    1610612745      96.0        0.450        0.682         0.409      21.0      51.0    1610612753      86.0        0.405        0.750         0.444      19.0      35.0               1\n",
      "19092    2003-11-08  20300078    1610612739       1610612764    2003    1610612739     111.0        0.524        0.677         0.250      30.0      42.0    1610612764      98.0        0.449        0.667         0.250      22.0      38.0               1\n",
      "19091    2003-11-08  20300082    1610612759       1610612742    2003    1610612759      78.0        0.325        0.808         0.188      15.0      49.0    1610612742      81.0        0.400        0.533         0.250      18.0      45.0               0\n",
      "19086    2003-11-09  20300088    1610612765       1610612751    2003    1610612765      98.0        0.438        0.758         0.300      25.0      41.0    1610612751      84.0        0.432        0.750         0.222      25.0      37.0               1\n",
      "19088    2003-11-09  20300085    1610612761       1610612743    2003    1610612761      89.0        0.487        0.769         0.385      25.0      41.0    1610612743      76.0        0.347        0.846         0.286      11.0      39.0               1\n",
      "\n",
      "[200 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "games_df = games_df.drop(columns=['GAME_STATUS_TEXT'])\n",
    "\n",
    "games_df.groupby(['SEASON'])\n",
    "games_df = games_df.sort_values('GAME_DATE_EST')\n",
    "\n",
    "# games_df = games_df.dropna()\n",
    "\n",
    "print(games_df.head(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create the ELO rating for each team, as a default the team will start with 1500 points. Each time a team wins or loses a game, the ELO rating will be updated. The ELO rating will be updated based on the following formula:\n",
    "\n",
    "\n",
    "\n",
    "Updated Team ELO = Team ELO + k * (Team Expected Outcome - Team Actual Outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_elo = 1500\n",
    "k_factor = 15\n",
    "team_elos = {team_id: initial_elo for team_id in pd.concat([games_df['HOME_TEAM_ID'], games_df['VISITOR_TEAM_ID']]).unique()}\n",
    "\n",
    "\n",
    "def expected_outcome(home_elo, away_elo):\n",
    "    return 1 / (1 + 10 ** ((away_elo - home_elo) / 400))\n",
    "\n",
    "def update_elo(home_elo, visitor_elo, home_win, k_factor):\n",
    "    expected_home_win = expected_outcome(home_elo, visitor_elo)\n",
    "    actual_home_win = 1 if home_win else 0\n",
    "    new_home_elo = home_elo + k_factor * (actual_home_win - expected_home_win)\n",
    "    new_visitor_elo = visitor_elo + k_factor * ((1 - actual_home_win) - (1 - expected_home_win))\n",
    "\n",
    "    return new_home_elo, new_visitor_elo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating, team_elos will have the updated Elo ratings for each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = games_df.sort_values('GAME_DATE_EST', ascending=False)\n",
    "games_df['ELO_home'] = 0\n",
    "games_df['ELO_away'] = 0\n",
    "\n",
    "for index, row in games_df.iterrows():\n",
    "    home_team, away_team = row['HOME_TEAM_ID'], row['VISITOR_TEAM_ID']\n",
    "    home_elo, away_elo = team_elos[home_team], team_elos[away_team]\n",
    "    home_win = row['HOME_TEAM_WINS']\n",
    "    new_home_elo, new_away_elo = update_elo(home_elo, away_elo, home_win, k_factor)\n",
    "    games_df.at[index, 'ELO_home'] = round(new_home_elo)\n",
    "    games_df.at[index, 'ELO_away'] = round(new_away_elo)\n",
    "    \n",
    "    team_elos[home_team], team_elos[away_team] = new_home_elo, new_away_elo\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group each Season into its own dataframe, this will help for calculating the overall ELO rating for each team in a season. I can then also track the ELO rating for each team over time, and see how it changes over the course of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME_DATE_EST      0.000000\n",
      "GAME_ID            0.000000\n",
      "HOME_TEAM_ID       0.000000\n",
      "VISITOR_TEAM_ID    0.000000\n",
      "SEASON             0.000000\n",
      "TEAM_ID_home       0.000000\n",
      "PTS_home           7.148014\n",
      "FG_PCT_home        7.148014\n",
      "FT_PCT_home        7.148014\n",
      "FG3_PCT_home       7.148014\n",
      "AST_home           7.148014\n",
      "REB_home           7.148014\n",
      "TEAM_ID_away       0.000000\n",
      "PTS_away           7.148014\n",
      "FG_PCT_away        7.148014\n",
      "FT_PCT_away        7.148014\n",
      "FG3_PCT_away       7.148014\n",
      "AST_away           7.148014\n",
      "REB_away           7.148014\n",
      "HOME_TEAM_WINS     0.000000\n",
      "ELO_home           0.000000\n",
      "ELO_away           0.000000\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n",
      "GAME_DATE_EST      0.0\n",
      "GAME_ID            0.0\n",
      "HOME_TEAM_ID       0.0\n",
      "VISITOR_TEAM_ID    0.0\n",
      "SEASON             0.0\n",
      "TEAM_ID_home       0.0\n",
      "PTS_home           0.0\n",
      "FG_PCT_home        0.0\n",
      "FT_PCT_home        0.0\n",
      "FG3_PCT_home       0.0\n",
      "AST_home           0.0\n",
      "REB_home           0.0\n",
      "TEAM_ID_away       0.0\n",
      "PTS_away           0.0\n",
      "FG_PCT_away        0.0\n",
      "FT_PCT_away        0.0\n",
      "FG3_PCT_away       0.0\n",
      "AST_away           0.0\n",
      "REB_away           0.0\n",
      "HOME_TEAM_WINS     0.0\n",
      "ELO_home           0.0\n",
      "ELO_away           0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "seasons_df = games_df.groupby('SEASON')\n",
    "seasons_dict = {}\n",
    "for season, season_df in seasons_df:\n",
    "    seasons_dict[season] = season_df\n",
    "\n",
    "# Iterate over the dictionary and calculate the percentage of missing values\n",
    "for season, season_df in seasons_dict.items():\n",
    "    missing_values_percent = season_df.isnull().sum() * 100 / len(season_df)\n",
    "    # print(f\"Percentage of missing values for season {season}:\")\n",
    "    print(missing_values_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each team's ELO will be saved in a dataframe then merged with the team dataframe to get the ELO rating for each team in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_elos_df = pd.DataFrame.from_dict(team_elos, orient=\"index\", columns=[\"ELO\"])\n",
    "team_elos_df = team_elos_df.merge(teams_df, left_index=True, right_on=\"TEAM_ID\")\n",
    "team_elos_df = team_elos_df.sort_values(\"ELO\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of close games: 5631\n",
      "Probability of a close game: 21.20744200060259\n"
     ]
    }
   ],
   "source": [
    "# Create margin of victory column\n",
    "games_df[\"MOV\"] = games_df[\"PTS_home\"] - games_df[\"PTS_away\"]\n",
    "close_games = games_df[(games_df[\"MOV\"] > -5) & (games_df[\"MOV\"] < 5)]\n",
    "print(\"Number of close games:\", close_games.shape[0])\n",
    "\n",
    "close_game_prob = close_games[\"MOV\"].count() / games_df[\"MOV\"].count() * 100\n",
    "print(\"Probability of a close game:\", close_game_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high scoring games: 6258\n",
      "Probability of a high scoring game: 23.48129526096582\n"
     ]
    }
   ],
   "source": [
    "# Create high scoring game column\n",
    "games_df[\"total_score\"] = games_df[\"PTS_home\"] + games_df[\"PTS_away\"]\n",
    "high_scoring_games = games_df[games_df[\"total_score\"] > 220]\n",
    "print(\"Number of high scoring games:\", high_scoring_games.shape[0])\n",
    "\n",
    "high_scoring_game_prob = len(high_scoring_games) / len(games_df) * 100\n",
    "print(\"Probability of a high scoring game:\", high_scoring_game_prob)\n",
    "\n",
    "games_df = games_df.dropna(ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, losses):\n",
    "    average_loss = round(np.average(losses), 3)\n",
    "    model_to_string = model.__class__.__name__\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if not os.path.exists(f'./models/{model_to_string}'):\n",
    "        os.makedirs(f'./models/{model_to_string}')\n",
    "    torch.save(model.state_dict(), f=f'./models/{model_to_string}/{average_loss}_on_{timestamp}_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will stop training runs if the training loss is not decreasing anymore. This is done through comparing the current loss with the previous loss. If the loss is not decreasing, the training will stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Creation\n",
    "\n",
    "The model is created, experimenting with the different model architecture. The model will be trained on the first 80% of the data, and tested on the last 20% of the data. The model will be evaluated based on the accuracy of the predictions. \n",
    "\n",
    "The model will be trained on the ELO rating of each team, and the difference in ELO rating between the two teams. The model will predict the outcome of the game based on the ELO rating of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "    features_df = games_df[\n",
    "            [\n",
    "                # \"PTS_home\",\n",
    "                # \"PTS_away\",\n",
    "                # \"FG_PCT_home\",\n",
    "                # \"FT_PCT_home\",\n",
    "                # \"FG3_PCT_home\",\n",
    "                # \"AST_home\",\n",
    "                # \"REB_home\",\n",
    "                # \"FG_PCT_away\",\n",
    "                # \"FT_PCT_away\",\n",
    "                # \"FG3_PCT_away\",\n",
    "                # \"AST_away\",\n",
    "                # \"REB_away\",\n",
    "                \"SEASON\",\n",
    "                # \"MOV\",\n",
    "                \"total_score\",\n",
    "                \"ELO_home\",\n",
    "                \"ELO_away\",\n",
    "                \"HOME_TEAM_ID\",\n",
    "                \"VISITOR_TEAM_ID\",]\n",
    "        ]\n",
    "\n",
    "\n",
    "    class BinaryClassifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifier, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=6, out_features=50)\n",
    "            self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.prelu(self.fc1(x), torch.tensor(0.75))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "    features = torch.tensor(\n",
    "        features_df.values,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    labels = torch.tensor(games_df[\"HOME_TEAM_WINS\"])\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    model = BinaryClassifier()\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    num_epochs = 10\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "else: \n",
    "    print(\"Cell skipped\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model v2 - relu activation function and larger hidden layers\n",
    "Here I have created a model with larger hidden layers, and more hidden layers, to see if the model can learn more complex patterns in the data. The learning rate is lowered and a scheduler is added to help the model converge to a better solution. The model is trained for 100 epochs, and the loss rate is assessed to see if the model is learning at a better rate than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 26.033058166503906\n",
      "Epoch 2, Loss: 32.85123825073242\n",
      "Epoch 3, Loss: 35.95041275024414\n",
      "Epoch 4, Loss: 30.991735458374023\n",
      "Epoch 5, Loss: 31.611570358276367\n",
      "Epoch 6, Loss: 27.272727966308594\n",
      "Epoch 7, Loss: 29.132230758666992\n",
      "Epoch 8, Loss: 31.611570358276367\n",
      "Epoch 9, Loss: 34.71074295043945\n",
      "Epoch 10, Loss: 31.611570358276367\n",
      "Epoch 11, Loss: 22.933883666992188\n",
      "Epoch 12, Loss: 30.37190055847168\n",
      "Epoch 13, Loss: 26.033058166503906\n",
      "Epoch 14, Loss: 27.272727966308594\n",
      "Epoch 15, Loss: 29.752065658569336\n",
      "Epoch 16, Loss: 29.132230758666992\n",
      "Epoch 17, Loss: 29.132230758666992\n",
      "Epoch 18, Loss: 33.471073150634766\n",
      "Epoch 19, Loss: 28.51239585876465\n",
      "Epoch 20, Loss: 29.132230758666992\n",
      "Epoch 21, Loss: 29.752065658569336\n",
      "Epoch 22, Loss: 24.173553466796875\n",
      "Epoch 23, Loss: 32.85123825073242\n",
      "Epoch 24, Loss: 37.80991744995117\n",
      "Epoch 25, Loss: 35.3305778503418\n",
      "Epoch 26, Loss: 32.85123825073242\n",
      "Epoch 27, Loss: 38.429752349853516\n",
      "Epoch 28, Loss: 29.132230758666992\n",
      "Epoch 29, Loss: 28.51239585876465\n",
      "Epoch 30, Loss: 33.471073150634766\n",
      "Epoch 31, Loss: 24.79338836669922\n",
      "Epoch 32, Loss: 30.37190055847168\n",
      "Epoch 33, Loss: 35.3305778503418\n",
      "Epoch 34, Loss: 26.65289306640625\n",
      "Epoch 35, Loss: 29.132230758666992\n",
      "Epoch 36, Loss: 32.85123825073242\n",
      "Epoch 37, Loss: 26.65289306640625\n",
      "Epoch 38, Loss: 29.132230758666992\n",
      "Epoch 39, Loss: 34.71074295043945\n",
      "Epoch 40, Loss: 27.272727966308594\n",
      "Epoch 41, Loss: 29.752065658569336\n",
      "Epoch 42, Loss: 29.132230758666992\n",
      "Epoch 43, Loss: 26.65289306640625\n",
      "Epoch 44, Loss: 39.04958724975586\n",
      "Epoch 45, Loss: 33.471073150634766\n",
      "Epoch 46, Loss: 27.272727966308594\n",
      "Epoch 47, Loss: 24.173553466796875\n",
      "Epoch 48, Loss: 27.272727966308594\n",
      "Epoch 49, Loss: 31.611570358276367\n",
      "Epoch 50, Loss: 28.51239585876465\n",
      "Epoch 51, Loss: 31.611570358276367\n",
      "Epoch 52, Loss: 27.892562866210938\n",
      "Epoch 53, Loss: 32.85123825073242\n",
      "Epoch 54, Loss: 32.85123825073242\n",
      "Epoch 55, Loss: 32.23140335083008\n",
      "Epoch 56, Loss: 30.37190055847168\n",
      "Epoch 57, Loss: 32.23140335083008\n",
      "Epoch 58, Loss: 24.79338836669922\n",
      "Epoch 59, Loss: 34.09090805053711\n",
      "Epoch 60, Loss: 29.132230758666992\n",
      "Epoch 61, Loss: 27.892562866210938\n",
      "Epoch 62, Loss: 34.71074295043945\n",
      "Epoch 63, Loss: 30.991735458374023\n",
      "Epoch 64, Loss: 32.85123825073242\n",
      "Epoch 65, Loss: 30.37190055847168\n",
      "Epoch 66, Loss: 32.85123825073242\n",
      "Epoch 67, Loss: 28.51239585876465\n",
      "Epoch 68, Loss: 34.09090805053711\n",
      "Epoch 69, Loss: 32.85123825073242\n",
      "Epoch 70, Loss: 27.272727966308594\n",
      "Epoch 71, Loss: 32.23140335083008\n",
      "Epoch 72, Loss: 27.272727966308594\n",
      "Epoch 73, Loss: 24.79338836669922\n",
      "Epoch 74, Loss: 35.3305778503418\n",
      "Epoch 75, Loss: 32.85123825073242\n",
      "Epoch 76, Loss: 29.132230758666992\n",
      "Epoch 77, Loss: 23.55371856689453\n",
      "Epoch 78, Loss: 30.991735458374023\n",
      "Epoch 79, Loss: 32.85123825073242\n",
      "Epoch 80, Loss: 31.611570358276367\n",
      "Epoch 81, Loss: 29.132230758666992\n",
      "Epoch 82, Loss: 34.71074295043945\n",
      "Epoch 83, Loss: 31.611570358276367\n",
      "Epoch 84, Loss: 32.23140335083008\n",
      "Epoch 85, Loss: 31.611570358276367\n",
      "Epoch 86, Loss: 26.65289306640625\n",
      "Epoch 87, Loss: 29.132230758666992\n",
      "Epoch 88, Loss: 26.033058166503906\n",
      "Epoch 89, Loss: 27.892562866210938\n",
      "Epoch 90, Loss: 30.37190055847168\n",
      "Epoch 91, Loss: 23.55371856689453\n",
      "Epoch 92, Loss: 29.132230758666992\n",
      "Epoch 93, Loss: 30.37190055847168\n",
      "Epoch 94, Loss: 30.37190055847168\n",
      "Epoch 95, Loss: 34.09090805053711\n",
      "Epoch 96, Loss: 26.65289306640625\n",
      "Epoch 97, Loss: 29.132230758666992\n",
      "Epoch 98, Loss: 32.23140335083008\n",
      "Epoch 99, Loss: 30.37190055847168\n",
      "Epoch 100, Loss: 39.04958724975586\n"
     ]
    }
   ],
   "source": [
    "skip_cell = False\n",
    "if not skip_cell:\n",
    "    features_df = games_df[\n",
    "            [\n",
    "                \"SEASON\",\n",
    "                \"total_score\",\n",
    "                \"ELO_home\",\n",
    "                \"ELO_away\",\n",
    "                \"HOME_TEAM_ID\",\n",
    "                \"VISITOR_TEAM_ID\",]\n",
    "        ]\n",
    "\n",
    "    class BinaryClassifierV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=6, out_features=1000)\n",
    "            self.fc2 = nn.Linear(1000, 50)\n",
    "            self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "    features = torch.tensor(\n",
    "        features_df.values,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    labels = torch.tensor(games_df[\"HOME_TEAM_WINS\"])\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    model = BinaryClassifierV2()\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', cooldown=5, patience=2)\n",
    "\n",
    "    num_epochs = 100\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        \n",
    "    save_model(model, losses)\n",
    "\n",
    "else: \n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model v3.1 - More model information\n",
    "The results from the model now include things like the F1 score, precision, recall, and the confusion matrix. This will help to better understand the performance of the model, and see where the model is making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "    class BinaryClassifierV3(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierV3, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=6, out_features=1000)\n",
    "            self.fc2 = nn.Linear(1000, 50)\n",
    "            self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "    features_df = games_df[\n",
    "        [\n",
    "            \"SEASON\",\n",
    "            \"total_score\",\n",
    "            \"ELO_home\",\n",
    "            \"ELO_away\",\n",
    "            \"HOME_TEAM_ID\",\n",
    "            \"VISITOR_TEAM_ID\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    features = torch.tensor(features_df.values, dtype=torch.float32)\n",
    "    labels = torch.tensor(games_df[\"HOME_TEAM_WINS\"].values, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    model = BinaryClassifierV3()\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', cooldown=5, patience=2)\n",
    "\n",
    "    num_epochs = 100\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_targets = []\n",
    "        train_predictions = []\n",
    "        \n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Collect targets and predictions for metrics\n",
    "            train_targets.extend(targets.numpy())\n",
    "            train_predictions.extend(outputs.detach().numpy())\n",
    "        \n",
    "        # Calculate metrics for training data\n",
    "        train_predictions_binary = [1 if pred > 0.5 else 0 for pred in train_predictions]\n",
    "        train_accuracy = accuracy_score(train_targets, train_predictions_binary)\n",
    "        train_precision = precision_score(train_targets, train_predictions_binary)\n",
    "        train_recall = recall_score(train_targets, train_predictions_binary)\n",
    "        train_f1 = f1_score(train_targets, train_predictions_binary)\n",
    "        train_roc_auc = roc_auc_score(train_targets, train_predictions)\n",
    "        train_conf_matrix = confusion_matrix(train_targets, train_predictions_binary)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        print(f\"Train Accuracy: {train_accuracy}\")\n",
    "        print(f\"Train Precision: {train_precision}\")\n",
    "        print(f\"Train Recall: {train_recall}\")\n",
    "        print(f\"Train F1-Score: {train_f1}\")\n",
    "        print(f\"Train ROC-AUC: {train_roc_auc}\")\n",
    "        print(f\"Train Confusion Matrix:\\n{train_conf_matrix}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_targets = []\n",
    "        val_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                \n",
    "                # Collect targets and predictions for metrics\n",
    "                val_targets.extend(targets.numpy())\n",
    "                val_predictions.extend(outputs.numpy())\n",
    "        \n",
    "        # Calculate metrics for validation data\n",
    "        val_predictions_binary = [1 if pred > 0.5 else 0 for pred in val_predictions]\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions_binary)\n",
    "        val_precision = precision_score(val_targets, val_predictions_binary)\n",
    "        val_recall = recall_score(val_targets, val_predictions_binary)\n",
    "        val_f1 = f1_score(val_targets, val_predictions_binary)\n",
    "        val_roc_auc = roc_auc_score(val_targets, val_predictions)\n",
    "        val_conf_matrix = confusion_matrix(val_targets, val_predictions_binary)\n",
    "        \n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        print(f\"Validation Precision: {val_precision}\")\n",
    "        print(f\"Validation Recall: {val_recall}\")\n",
    "        print(f\"Validation F1-Score: {val_f1}\")\n",
    "        print(f\"Validation ROC-AUC: {val_roc_auc}\")\n",
    "        print(f\"Validation Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "\n",
    "    save_model()\n",
    "else:\n",
    "    print(\"Cell skipped\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model - Adding Dropout Layers\n",
    "\n",
    "Here I have added dropout layers to the model to help prevent overfitting. The dropout rate is set to 0.5, increasing layer count to 7 and increasing the number of neurons in each layer. The model is evaluated based on the loss rate, and the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "    features_df = games_df[\n",
    "            [\n",
    "                \"SEASON\",\n",
    "                \"total_score\",\n",
    "                \"ELO_home\",\n",
    "                \"ELO_away\",\n",
    "                \"HOME_TEAM_ID\",\n",
    "                \"VISITOR_TEAM_ID\",]\n",
    "        ]\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = games_df[\"HOME_TEAM_WINS\"].value_counts()\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = class_weights[games_df[\"HOME_TEAM_WINS\"]]\n",
    "\n",
    "    # Use weighted loss function\n",
    "\n",
    "    class BinaryClassifierDropout(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropout, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=6, out_features=1000)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1000, 1000)\n",
    "            self.dropout2 = nn.Dropout(p=0.5)\n",
    "            self.fc3 = nn.Linear(1000, 1000)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(1000, 1000)\n",
    "            self.dropout4 = nn.Dropout(p=0.5)\n",
    "            self.fc5 = nn.Linear(1000, 1000)\n",
    "            self.dropout5 = nn.Dropout(p=0.5)\n",
    "            self.fc6 = nn.Linear(1000, 500)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(500, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout1(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.dropout3(x)\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.relu(self.fc5(x))\n",
    "            x = self.dropout5(x)\n",
    "            x = F.relu(self.fc6(x))\n",
    "            x = self.dropout6(x)\n",
    "            x = torch.sigmoid(self.fc7(x))\n",
    "            return x\n",
    "\n",
    "    features = torch.tensor(\n",
    "        features_df.values,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    labels = torch.tensor(games_df[\"HOME_TEAM_WINS\"])\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    model = BinaryClassifierDropout()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', cooldown=5, patience=2)\n",
    "\n",
    "    num_epochs = 100\n",
    "    losses = []\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_targets = []\n",
    "        train_predictions = []\n",
    "        \n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze() \n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Collect targets and predictions for metrics\n",
    "            train_targets.extend(targets.numpy())\n",
    "            train_predictions.extend(outputs.detach().numpy())\n",
    "        \n",
    "        # Calculate metrics for training data\n",
    "        train_predictions_binary = [1 if pred > 0.5 else 0 for pred in train_predictions]\n",
    "        train_accuracy = accuracy_score(train_targets, train_predictions_binary)\n",
    "        train_precision = precision_score(train_targets, train_predictions_binary, zero_division=0.0)\n",
    "        train_recall = recall_score(train_targets, train_predictions_binary)\n",
    "        train_f1 = f1_score(train_targets, train_predictions_binary)\n",
    "        train_roc_auc = roc_auc_score(train_targets, train_predictions)\n",
    "        train_conf_matrix = confusion_matrix(train_targets, train_predictions_binary)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "            print(f\"Train Accuracy: {train_accuracy}\")\n",
    "            print(f\"Train Precision: {train_precision}\")\n",
    "            print(f\"Train Recall: {train_recall}\")\n",
    "            print(f\"Train F1-Score: {train_f1}\")\n",
    "            print(f\"Train ROC-AUC: {train_roc_auc}\")\n",
    "            print(f\"Train Confusion Matrix:\\n{train_conf_matrix}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_targets = []\n",
    "        val_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                \n",
    "                # Collect targets and predictions for metrics\n",
    "                val_targets.extend(targets.numpy())\n",
    "                val_predictions.extend(outputs.numpy())\n",
    "        \n",
    "        # Calculate metrics for validation data\n",
    "        val_predictions_binary = [1 if pred > 0.5 else 0 for pred in val_predictions]\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions_binary)\n",
    "        val_precision = precision_score(val_targets, val_predictions_binary, zero_division=0.0)\n",
    "        val_recall = recall_score(val_targets, val_predictions_binary)\n",
    "        val_f1 = f1_score(val_targets, val_predictions_binary)\n",
    "        val_roc_auc = roc_auc_score(val_targets, val_predictions)\n",
    "        val_conf_matrix = confusion_matrix(val_targets, val_predictions_binary)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "            print(f\"Validation Precision: {val_precision}\")\n",
    "            print(f\"Validation Recall: {val_recall}\")\n",
    "            print(f\"Validation F1-Score: {val_f1}\")\n",
    "            print(f\"Validation ROC-AUC: {val_roc_auc}\")\n",
    "            print(f\"Validation Confusion Matrix:\\n{val_conf_matrix}\")\n",
    "            \n",
    "        val_loss = criterion(torch.tensor(val_predictions), torch.tensor(val_targets).float()).item()\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "    save_model(model, losses)\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a rough outline of the final recommender system that I will be building in this notebook. It takes in the schedule for upcoming games, and outputs the recommended games that will be the closest and most exciting to watch based on the historical data from NBA games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Load the trained model\n",
    "# model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "# model.load('nba_game_recommender.pkl')\n",
    "\n",
    "# # Load the schedule for the upcoming night's games\n",
    "# schedule = pd.read_csv('nba_schedule.csv')\n",
    "\n",
    "# # Extract relevant features for each game\n",
    "# features = []\n",
    "# for index, row in schedule.iterrows():\n",
    "#     game_id = row['Game_ID']\n",
    "#     home_team = row['Home_Team']\n",
    "#     away_team = row['Away_Team']\n",
    "#     # Extract features from historical data\n",
    "#     feature_vector = get_features(game_id, home_team, away_team)\n",
    "#     features.append(feature_vector)\n",
    "\n",
    "# # Use the model to predict the most exciting game(s) to watch\n",
    "# distances, indices = model.kneighbors(features)\n",
    "# recommended_games = []\n",
    "# for i, dist in enumerate(distances):\n",
    "#     if dist < 0.5:  # Arbitrarily set a threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
