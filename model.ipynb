{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    WeightedRandomSampler,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# change the display width to see all columns\n",
    "pd.set_option(\"display.width\", 500)\n",
    "\n",
    "games_df = pd.read_csv(\"./data/games.csv\", low_memory=False)\n",
    "games_details_df = pd.read_csv(\"./data/games_details.csv\", low_memory=False)\n",
    "# players_df = pd.read_csv('./data/players.csv')\n",
    "ranking_df = pd.read_csv(\"./data/ranking.csv\")\n",
    "teams_df = pd.read_csv(\"./data/teams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the game dataframe and group by season, create two new columns in the games_df one for the home teams previous 10 games and one for the away teams previous 10 games. The values are up to 10, 1 for each win, and 0 for each loss. The games are sorted by date so the games are in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME_TEAM_WINS\n",
      "1    62988\n",
      "0    44312\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "games_df = games_df.drop(columns=[\"GAME_STATUS_TEXT\"])\n",
    "\n",
    "games_df.groupby([\"SEASON\"])\n",
    "games_df = games_df.sort_values(by=[\"HOME_TEAM_ID\", \"GAME_DATE_EST\"])\n",
    "home_games = games_df[\n",
    "    [\"GAME_ID\", \"HOME_TEAM_ID\", \"GAME_DATE_EST\", \"HOME_TEAM_WINS\"]\n",
    "].rename(columns={\"HOME_TEAM_ID\": \"team_id\", \"HOME_TEAM_WINS\": \"win\"})\n",
    "away_games = games_df[\n",
    "    [\"GAME_ID\", \"VISITOR_TEAM_ID\", \"GAME_DATE_EST\", \"HOME_TEAM_WINS\"]\n",
    "].rename(columns={\"VISITOR_TEAM_ID\": \"team_id\", \"HOME_TEAM_WINS\": \"win\"})\n",
    "away_games[\"win\"] = 1 - away_games[\"win\"]\n",
    "all_games = pd.concat([home_games, away_games]).sort_values(\n",
    "    by=[\"team_id\", \"GAME_DATE_EST\"]\n",
    ")\n",
    "all_games[\"rolling_wins\"] = (\n",
    "    all_games.groupby(\"team_id\")[\"win\"]\n",
    "    .rolling(window=10, min_periods=1)\n",
    "    .sum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "all_games[\"prev_10_game_record\"] = all_games.groupby(\"team_id\")[\"rolling_wins\"].shift(1)\n",
    "all_games[\"prev_10_game_record\"] = all_games[\"prev_10_game_record\"].fillna(0)\n",
    "home_records = all_games[\n",
    "    all_games[\"GAME_ID\"].isin(games_df[\"GAME_ID\"])\n",
    "    & (all_games[\"team_id\"].isin(games_df[\"HOME_TEAM_ID\"]))\n",
    "]\n",
    "away_records = all_games[\n",
    "    all_games[\"GAME_ID\"].isin(games_df[\"GAME_ID\"])\n",
    "    & (all_games[\"team_id\"].isin(games_df[\"VISITOR_TEAM_ID\"]))\n",
    "]\n",
    "games_df = games_df.merge(\n",
    "    home_records[[\"GAME_ID\", \"prev_10_game_record\"]],\n",
    "    left_on=\"GAME_ID\",\n",
    "    right_on=\"GAME_ID\",\n",
    "    how=\"left\",\n",
    ").rename(columns={\"prev_10_game_record\": \"HOME_TEAM_L10\"})\n",
    "games_df = games_df.merge(\n",
    "    away_records[[\"GAME_ID\", \"prev_10_game_record\"]],\n",
    "    left_on=\"GAME_ID\",\n",
    "    right_on=\"GAME_ID\",\n",
    "    how=\"left\",\n",
    ").rename(columns={\"prev_10_game_record\": \"AWAY_TEAM_L10\"})\n",
    "\n",
    "print(games_df[\"HOME_TEAM_WINS\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create the ELO rating for each team, as a default the team will start with 1500 points. Each time a team wins or loses a game, the ELO rating will be updated. The ELO rating will be updated based on the following formula:\n",
    "\n",
    "\n",
    "\n",
    "Updated Team ELO = Team ELO + k * (Team Expected Outcome - Team Actual Outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_elo = 1500\n",
    "k_factor = 15\n",
    "team_elos = {\n",
    "    team_id: initial_elo\n",
    "    for team_id in pd.concat(\n",
    "        [games_df[\"HOME_TEAM_ID\"], games_df[\"VISITOR_TEAM_ID\"]]\n",
    "    ).unique()\n",
    "}\n",
    "\n",
    "\n",
    "def expected_outcome(home_elo, away_elo):\n",
    "    return 1 / (1 + 10 ** ((away_elo - home_elo) / 400))\n",
    "\n",
    "\n",
    "def update_elo(home_elo, visitor_elo, home_win, k_factor):\n",
    "    expected_home_win = expected_outcome(home_elo, visitor_elo)\n",
    "    actual_home_win = 1 if home_win else 0\n",
    "    new_home_elo = home_elo + k_factor * (actual_home_win - expected_home_win)\n",
    "    new_visitor_elo = visitor_elo + k_factor * (\n",
    "        (1 - actual_home_win) - (1 - expected_home_win)\n",
    "    )\n",
    "\n",
    "    return new_home_elo, new_visitor_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating, team_elos will have the updated Elo ratings for each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = games_df.sort_values(\"GAME_DATE_EST\", ascending=False)\n",
    "games_df[\"ELO_home\"] = 0\n",
    "games_df[\"ELO_away\"] = 0\n",
    "\n",
    "for index, row in games_df.iterrows():\n",
    "    home_team, away_team = row[\"HOME_TEAM_ID\"], row[\"VISITOR_TEAM_ID\"]\n",
    "    home_elo, away_elo = team_elos[home_team], team_elos[away_team]\n",
    "    home_win = row[\"HOME_TEAM_WINS\"]\n",
    "    new_home_elo, new_away_elo = update_elo(home_elo, away_elo, home_win, k_factor)\n",
    "    games_df.at[index, \"ELO_home\"] = round(new_home_elo)\n",
    "    games_df.at[index, \"ELO_away\"] = round(new_away_elo)\n",
    "\n",
    "    team_elos[home_team], team_elos[away_team] = new_home_elo, new_away_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group each Season into its own dataframe, this will help for calculating the overall ELO rating for each team in a season. I can then also track the ELO rating for each team over time, and see how it changes over the course of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df = games_df.groupby(\"SEASON\")\n",
    "seasons_dict = {}\n",
    "for season, season_df in seasons_df:\n",
    "    seasons_dict[season] = season_df\n",
    "\n",
    "# Iterate over the dictionary and calculate the percentage of missing values\n",
    "for season, season_df in seasons_dict.items():\n",
    "    missing_values_percent = season_df.isnull().sum() * 100 / len(season_df)\n",
    "    # print(f\"Percentage of missing values for season {season}:\")\n",
    "    # print(missing_values_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each team's ELO will be saved in a dataframe then merged with the team dataframe to get the ELO rating for each team in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_elos_df = pd.DataFrame.from_dict(team_elos, orient=\"index\", columns=[\"ELO\"])\n",
    "team_elos_df = team_elos_df.merge(teams_df, left_index=True, right_on=\"TEAM_ID\")\n",
    "team_elos_df = team_elos_df.sort_values(\"ELO\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of close games: 22764\n",
      "Probability of a close game: 21.293871136720796\n"
     ]
    }
   ],
   "source": [
    "# Create margin of victory column\n",
    "games_df[\"MOV\"] = games_df[\"PTS_home\"] - games_df[\"PTS_away\"]\n",
    "close_games = games_df[(games_df[\"MOV\"] > -5) & (games_df[\"MOV\"] < 5)]\n",
    "print(\"Number of close games:\", close_games.shape[0])\n",
    "\n",
    "close_game_prob = close_games[\"MOV\"].count() / games_df[\"MOV\"].count() * 100\n",
    "print(\"Probability of a close game:\", close_game_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high scoring games: 25344\n",
      "Probability of a high scoring game: 23.619757688723205\n"
     ]
    }
   ],
   "source": [
    "# Create high scoring game column\n",
    "games_df[\"total_score\"] = games_df[\"PTS_home\"] + games_df[\"PTS_away\"]\n",
    "high_scoring_games = games_df[games_df[\"total_score\"] > 220]\n",
    "print(\"Number of high scoring games:\", high_scoring_games.shape[0])\n",
    "\n",
    "high_scoring_game_prob = len(high_scoring_games) / len(games_df) * 100\n",
    "print(\"Probability of a high scoring game:\", high_scoring_game_prob)\n",
    "\n",
    "games_df = games_df.dropna(ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model each time the model is trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, losses):\n",
    "    average_loss = round(np.average(losses), 3)\n",
    "    model_to_string = model.__class__.__name__\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if not os.path.exists(f\"./models/{model_to_string}\"):\n",
    "        os.makedirs(f\"./models/{model_to_string}\")\n",
    "    torch.save(\n",
    "        {\"model_state_dict\": model.state_dict(), \"losses\": losses},\n",
    "        f=f\"./models/{model_to_string}/{average_loss}_on_{timestamp}_model.pth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(targets, predictions):\n",
    "    predictions_binary = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "    accuracy = accuracy_score(targets, predictions_binary)\n",
    "    precision = precision_score(targets, predictions_binary, zero_division=0.0)\n",
    "    recall = recall_score(targets, predictions_binary)\n",
    "    f1 = f1_score(targets, predictions_binary)\n",
    "    roc_auc = roc_auc_score(targets, predictions)\n",
    "    conf_matrix = confusion_matrix(targets, predictions_binary)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"conf_matrix\": conf_matrix,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_metrics(phase, metrics):\n",
    "    print(f\"{phase} Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"{phase} Precision: {metrics['precision']}\")\n",
    "    print(f\"{phase} Recall: {metrics['recall']}\")\n",
    "    print(f\"{phase} F1-Score: {metrics['f1']}\")\n",
    "    print(f\"{phase} ROC-AUC: {metrics['roc_auc']}\")\n",
    "    print(f\"{phase} Confusion Matrix:\\n{metrics['conf_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will stop training runs if the training loss is not decreasing anymore. This is done through comparing the current loss with the previous loss. If the loss is not decreasing, the training will stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(outputs, threshold=0.5):\n",
    "    return (outputs > threshold).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training function, this function will train the model on the training data and return the model. The model will be trained using the training data and the labels. The model will be trained for a certain number of epochs and the loss will be calculated after each epoch. The loss will be used to determine if the model is improving or not. If the loss is not decreasing, the training will stop.\n",
    "\n",
    "Calculating and printing the metrics for the model, we will use the following metrics: accuracy, precision, recall, f1 score, and confusion matrix. These metrics will help us understand how well the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    validation_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    early_stopping,\n",
    "    num_epochs=100,\n",
    "):\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_targets = []\n",
    "        train_predictions = []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.float()\n",
    "            # Ensure targets are the correct shape\n",
    "            if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                targets = targets.float()\n",
    "            elif isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                targets = targets.float()\n",
    "            elif isinstance(criterion, nn.BCELoss):\n",
    "                targets = targets.float()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            train_targets.extend(targets.numpy())\n",
    "            train_predictions.extend(outputs.detach().numpy())\n",
    "\n",
    "        train_metrics = calculate_metrics(train_targets, train_predictions)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "            print_metrics(\"Train\", train_metrics)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_targets = []\n",
    "        val_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in validation_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "\n",
    "                # Ensure targets are the correct shape\n",
    "                if isinstance(criterion, torch.nn.BCEWithLogitsLoss):\n",
    "                    targets = targets.float()\n",
    "                elif isinstance(criterion, torch.nn.CrossEntropyLoss):\n",
    "                    targets = targets.float()\n",
    "                elif isinstance(criterion, nn.BCELoss):\n",
    "                    targets = targets.float()\n",
    "\n",
    "                val_targets.extend(targets.numpy())\n",
    "                val_predictions.extend(outputs.numpy())\n",
    "        val_predictions_np = np.array(val_predictions)\n",
    "        val_targets_np = np.array(val_targets)\n",
    "        val_metrics = calculate_metrics(val_targets, val_predictions)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print_metrics(\"Validation\", val_metrics)\n",
    "\n",
    "        val_loss = criterion(\n",
    "            torch.tensor(val_predictions_np),\n",
    "            torch.tensor(val_targets_np).float().squeeze(),\n",
    "        ).item()\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    save_model(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing function, this function will preprocess the data before training the model. The data will be split into training and testing data. The data will be normalized and the labels will be one-hot encoded. The data will be split into training and testing data using the train_test_split function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(labels):\n",
    "    # Calculate the number of samples for each class\n",
    "    class_sample_counts = labels.value_counts().sort_index().tolist()\n",
    "    # Calculate class weights: inverse of the number of samples for each class\n",
    "    class_weights = 1.0 / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "    # Assign a weight to each sample based on its class\n",
    "    sample_weights = torch.tensor(\n",
    "        [class_weights[label] for label in labels], dtype=torch.float\n",
    "    )\n",
    "    print(f\"Sample weights: {sample_weights[:5]}\")\n",
    "\n",
    "    # Create the WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def smote_fn(features_df, games_df):\n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        features_df,\n",
    "        games_df[\"HOME_TEAM_WINS\"],\n",
    "        test_size=0.2,\n",
    "        stratify=games_df[\"HOME_TEAM_WINS\"],\n",
    "    )\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE()\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Convert to tensors\n",
    "    features = torch.tensor(X_train_res.values, dtype=torch.float32)\n",
    "    labels = torch.tensor(y_train_res.values, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    return features, labels, X_val, y_val\n",
    "\n",
    "\n",
    "def prepare_data(games_df):\n",
    "    # Select relevant features\n",
    "    features_df = games_df[\n",
    "        [\n",
    "            \"SEASON\",\n",
    "            \"total_score\",\n",
    "            \"ELO_home\",\n",
    "            \"ELO_away\",\n",
    "            \"HOME_TEAM_ID\",\n",
    "            \"VISITOR_TEAM_ID\",\n",
    "            \"HOME_TEAM_L10\",\n",
    "            \"AWAY_TEAM_L10\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Apply SMOTE and split data\n",
    "    features, labels, X_val, y_val = smote_fn(features_df, games_df)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Split dataset into training and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Extract labels from the train dataset for balancing\n",
    "    train_labels = pd.Series([label.item() for _, label in train_dataset])\n",
    "    sampler = balance_dataset(train_labels)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, sampler=sampler)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Create DataLoader for validation set\n",
    "    val_features = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    val_labels = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Creation\n",
    "\n",
    "The model is created, experimenting with the different model architecture. The model will be trained on the first 80% of the data, and tested on the last 20% of the data. The model will be evaluated based on the accuracy of the predictions. \n",
    "\n",
    "The model will be trained on the ELO rating of each team, and the difference in ELO rating between the two teams. The model will predict the outcome of the game based on the ELO rating of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifier, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=50)\n",
    "            self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.prelu(self.fc1(x), torch.tensor(0.75))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            return x\n",
    "\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    model = BinaryClassifier()\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model v2 - relu activation function and larger hidden layers\n",
    "Here I have created a model with larger hidden layers, and more hidden layers, to see if the model can learn more complex patterns in the data. The learning rate is lowered and a scheduler is added to help the model converge to a better solution. The model is trained for 100 epochs, and the loss rate is assessed to see if the model is learning at a better rate than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=1000)\n",
    "            self.fc2 = nn.Linear(1000, 50)\n",
    "            self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierV2()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model v3.1 - More model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierV3(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierV3, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=1000)\n",
    "            self.fc2 = nn.Linear(1000, 50)\n",
    "            self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierV3()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    criterion = nn.BCELoss(weight=torch.tensor(0.75))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model - Adding Dropout Layers\n",
    "\n",
    "Here I have added dropout layers to the model to help prevent overfitting. The dropout rate is set to 0.5, increasing layer count to 7 and increasing the number of neurons in each layer. The model is evaluated based on the loss rate, and the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropout(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropout, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=1000)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1000, 1000)\n",
    "            self.dropout2 = nn.Dropout(p=0.5)\n",
    "            self.fc3 = nn.Linear(1000, 1000)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(1000, 1000)\n",
    "            self.dropout4 = nn.Dropout(p=0.5)\n",
    "            self.fc5 = nn.Linear(1000, 1000)\n",
    "            self.dropout5 = nn.Dropout(p=0.5)\n",
    "            self.fc6 = nn.Linear(1000, 500)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(500, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout1(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.dropout3(x)\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.relu(self.fc5(x))\n",
    "            x = self.dropout5(x)\n",
    "            x = F.relu(self.fc6(x))\n",
    "            x = self.dropout6(x)\n",
    "            x = torch.sigmoid(self.fc7(x))\n",
    "            return x\n",
    "\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    class_weights = [1.0, 1.0]\n",
    "    model = BinaryClassifierDropout()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Model - Adding Batch Normalization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropout(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropout, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=1000)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1000, 500)\n",
    "            self.dropout2 = nn.Dropout(p=0.2)\n",
    "            self.fc3 = nn.Linear(500, 500)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(500, 500)\n",
    "            self.dropout4 = nn.Dropout(p=0.7)\n",
    "            self.fc5 = nn.Linear(500, 500)\n",
    "            self.dropout5 = nn.Dropout(p=0.2)\n",
    "            self.fc6 = nn.Linear(500, 500)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(500, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            x = self.dropout1(x)\n",
    "            x = F.leaky_relu(self.fc2(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = F.leaky_relu(self.fc3(x))\n",
    "            x = self.dropout3(x)\n",
    "            x = F.leaky_relu(self.fc4(x))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.leaky_relu(self.fc5(x))\n",
    "            x = self.dropout5(x)\n",
    "            x = F.leaky_relu(self.fc6(x))\n",
    "            x = self.dropout6(x)\n",
    "            x = torch.sigmoid(self.fc7(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierDropout()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out model v2 - Altering dropout layers with different dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropoutV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropoutV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=1000)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(1000, 500)\n",
    "            self.dropout2 = nn.Dropout(p=0.2)\n",
    "            self.fc3 = nn.Linear(500, 500)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(500, 500)\n",
    "            self.dropout4 = nn.Dropout(p=0.7)\n",
    "            self.fc5 = nn.Linear(500, 500)\n",
    "            self.dropout5 = nn.Dropout(p=0.2)\n",
    "            self.fc6 = nn.Linear(500, 500)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(500, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = F.leaky_relu(self.fc2(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = F.leaky_relu(self.fc3(x))\n",
    "            x = self.dropout3(x)\n",
    "            x = F.leaky_relu(self.fc4(x))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.leaky_relu(self.fc5(x))\n",
    "            x = self.dropout5(x)\n",
    "            x = F.leaky_relu(self.fc6(x))\n",
    "            x = self.dropout6(x)\n",
    "            x = torch.sigmoid(self.fc7(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierDropoutV2()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    class_weights = [1.0, 1.0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropoutV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropoutV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=500)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(500, 64)\n",
    "            self.dropout2 = nn.Dropout(p=0.2)\n",
    "            self.fc3 = nn.Linear(64, 128)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(64, 128)\n",
    "            self.dropout4 = nn.Dropout(p=0.7)\n",
    "            self.fc5 = nn.Linear(64, 128)\n",
    "            self.dropout5 = nn.Dropout(p=0.2)\n",
    "            self.fc6 = nn.Linear(64, 128)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # print(f\"FC1{x.shape}\")\n",
    "            x = self.dropout1(x)\n",
    "            # print(f\"DO1{x.shape}\")\n",
    "            x = F.relu(self.fc2(x))\n",
    "            # print(f\"FC2{x.shape}\")\n",
    "            x = self.dropout2(x)\n",
    "            # print(f\"DO2{x.shape}\")\n",
    "            x = F.glu(self.fc3(x))\n",
    "            # print(f\"FC3 {x.shape}\")\n",
    "            x = self.dropout3(x)\n",
    "            # print(f\"DO3 {x.shape}\")\n",
    "            x = F.glu(self.fc4(x))\n",
    "            # print(f\"FC4 {x.shape}\")\n",
    "            x = self.dropout4(x)\n",
    "            # print(f\"DO4 {x.shape}\")\n",
    "            x = F.glu(self.fc5(x))\n",
    "            # print(f\"FC5 {x.shape}\")\n",
    "            x = self.dropout5(x)\n",
    "            # print(f\"DO5 {x.shape}\")\n",
    "            x = F.glu(self.fc6(x))\n",
    "            # print(f\"FC6 {x.shape}\")\n",
    "            x = self.dropout6(x)\n",
    "            # print(f\"DO6 {x.shape}\")\n",
    "            x = torch.sigmoid(self.fc7(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierDropoutV2()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    class_weights = [1.0, 1.0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(50, 0.001),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weights: tensor([2.4817e-05, 2.4817e-05, 2.4817e-05, 2.4817e-05, 2.4817e-05])\n",
      "Epoch 5, Loss: 0.7716470956802368\n",
      "Train Accuracy: 0.5003596943838063\n",
      "Train Precision: 0.4939476171601061\n",
      "Train Recall: 0.40615754901469187\n",
      "Train F1-Score: 0.44577136331741946\n",
      "Train ROC-AUC: 0.4984946302371942\n",
      "Train Confusion Matrix:\n",
      "[[24141 16597]\n",
      " [23686 16200]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 10, Loss: 0.7959917783737183\n",
      "Train Accuracy: 0.49967751538003574\n",
      "Train Precision: 0.4985958090300281\n",
      "Train Recall: 0.4014212239421572\n",
      "Train F1-Score: 0.44476256022023397\n",
      "Train ROC-AUC: 0.49960484614780154\n",
      "Train Confusion Matrix:\n",
      "[[24130 16247]\n",
      " [24091 16156]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 15, Loss: 0.7701418995857239\n",
      "Train Accuracy: 0.4997519349077198\n",
      "Train Precision: 0.4985615739165404\n",
      "Train Recall: 0.4005318223613907\n",
      "Train F1-Score: 0.44420251908607333\n",
      "Train ROC-AUC: 0.49993975054392337\n",
      "Train Confusion Matrix:\n",
      "[[24175 16210]\n",
      " [24122 16117]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 20, Loss: 0.7534114122390747\n",
      "Train Accuracy: 0.5009178408414368\n",
      "Train Precision: 0.501580578829451\n",
      "Train Recall: 0.40512134057162685\n",
      "Train F1-Score: 0.44822006472491904\n",
      "Train ROC-AUC: 0.5013587219865425\n",
      "Train Confusion Matrix:\n",
      "[[24043 16240]\n",
      " [23998 16343]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 25, Loss: 0.8187815546989441\n",
      "Train Accuracy: 0.49875967453859893\n",
      "Train Precision: 0.5020925652387986\n",
      "Train Recall: 0.40238729407122426\n",
      "Train F1-Score: 0.44674442801599035\n",
      "Train ROC-AUC: 0.49955799767902753\n",
      "Train Confusion Matrix:\n",
      "[[23896 16180]\n",
      " [24232 16316]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 30, Loss: 0.709263265132904\n",
      "Train Accuracy: 0.49968991863464973\n",
      "Train Precision: 0.49599482408035\n",
      "Train Recall: 0.4017017241809517\n",
      "Train F1-Score: 0.4438960501826704\n",
      "Train ROC-AUC: 0.5004613249368939\n",
      "Train Confusion Matrix:\n",
      "[[24188 16359]\n",
      " [23978 16099]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 35, Loss: 0.747921884059906\n",
      "Train Accuracy: 0.4972588807303036\n",
      "Train Precision: 0.49765258215962443\n",
      "Train Recall: 0.40163447251114415\n",
      "Train F1-Score: 0.44451753484356377\n",
      "Train ROC-AUC: 0.4969989607475626\n",
      "Train Confusion Matrix:\n",
      "[[23873 16371]\n",
      " [24162 16218]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 40, Loss: 0.7474088072776794\n",
      "Train Accuracy: 0.4990573526493352\n",
      "Train Precision: 0.4979601928544938\n",
      "Train Recall: 0.4002384737678855\n",
      "Train F1-Score: 0.44378339668374367\n",
      "Train ROC-AUC: 0.4994892653451718\n",
      "Train Confusion Matrix:\n",
      "[[24124 16244]\n",
      " [24144 16112]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 45, Loss: 0.7595263719558716\n",
      "Train Accuracy: 0.4991193689224052\n",
      "Train Precision: 0.49584368577650995\n",
      "Train Recall: 0.4029966094934184\n",
      "Train F1-Score: 0.4446247576086807\n",
      "Train ROC-AUC: 0.49869667944910245\n",
      "Train Confusion Matrix:\n",
      "[[24076 16436]\n",
      " [23947 16165]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Epoch 50, Loss: 0.8149997591972351\n",
      "Train Accuracy: 0.4964154594165509\n",
      "Train Precision: 0.49716248964692167\n",
      "Train Recall: 0.40100455265241486\n",
      "Train F1-Score: 0.4439361774977744\n",
      "Train ROC-AUC: 0.4960649975924874\n",
      "Train Confusion Matrix:\n",
      "[[23816 16392]\n",
      " [24209 16207]]\n",
      "Validation Accuracy: 0.5892147233525092\n",
      "Validation Precision: 0.5892147233525092\n",
      "Validation Recall: 1.0\n",
      "Validation F1-Score: 0.7415168192118662\n",
      "Validation ROC-AUC: 0.5\n",
      "Validation Confusion Matrix:\n",
      "[[    0  8783]\n",
      " [    0 12598]]\n",
      "Early stopping at epoch 51\n"
     ]
    }
   ],
   "source": [
    "skip_cell = True\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropoutV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropoutV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=64)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.dropout2 = nn.Dropout(p=0.2)\n",
    "            self.fc3 = nn.Linear(32, 64)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(32, 64)\n",
    "            self.dropout4 = nn.Dropout(p=0.7)\n",
    "            self.fc5 = nn.Linear(32, 64)\n",
    "            self.dropout5 = nn.Dropout(p=0.2)\n",
    "            self.fc6 = nn.Linear(32, 64)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(32, 64)\n",
    "            self.dropout7 = nn.Dropout(p=0.7)\n",
    "            self.fc8 = nn.Linear(32, 64)\n",
    "            self.dropout8 = nn.Dropout(p=0.2)\n",
    "            self.fc9 = nn.Linear(32, 64)\n",
    "            self.dropout9 = nn.Dropout(p=0.5)\n",
    "            self.fc10 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # print(f\"FC1{x.shape}\")\n",
    "            x = self.dropout1(x)\n",
    "            # print(f\"DO1{x.shape}\")\n",
    "            x = F.relu(self.fc2(x))\n",
    "            # print(f\"FC2{x.shape}\")\n",
    "            x = self.dropout2(x)\n",
    "            # print(f\"DO2{x.shape}\")\n",
    "            x = F.glu(self.fc3(x))\n",
    "            # print(f\"FC3 {x.shape}\")\n",
    "            x = self.dropout3(x)\n",
    "            # print(f\"DO3 {x.shape}\")\n",
    "            x = F.glu(self.fc4(x))\n",
    "            # print(f\"FC4 {x.shape}\")\n",
    "            x = self.dropout4(x)\n",
    "            # print(f\"DO4 {x.shape}\")\n",
    "            x = F.glu(self.fc5(x))\n",
    "            # print(f\"FC5 {x.shape}\")\n",
    "            x = self.dropout5(x)\n",
    "            # print(f\"DO5 {x.shape}\")\n",
    "            x = F.glu(self.fc6(x))\n",
    "            # print(f\"FC6 {x.shape}\")\n",
    "            x = self.dropout6(x)\n",
    "            # print(f\"DO6 {x.shape}\")\n",
    "            x = F.glu(self.fc7(x))\n",
    "            # print(f\"FC7 {x.shape}\")\n",
    "            x = self.dropout7(x)\n",
    "            # print(f\"DO7 {x.shape}\")\n",
    "            x = F.glu(self.fc8(x))\n",
    "            # print(f\"FC8 {x.shape}\")\n",
    "            x = self.dropout8(x)\n",
    "            # print(f\"DO8 {x.shape}\")\n",
    "            x = F.glu(self.fc9(x))\n",
    "            # print(f\"FC9 {x.shape}\")\n",
    "            x = self.dropout9(x)\n",
    "            # print(f\"DO9 {x.shape}\")\n",
    "            x = torch.sigmoid(self.fc10(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierDropoutV2()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    class_weights = [1.0, 1.0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(50, 0.001),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_cell = False\n",
    "\n",
    "if not skip_cell:\n",
    "\n",
    "    class BinaryClassifierDropoutV2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassifierDropoutV2, self).__init__()\n",
    "            self.fc1 = nn.Linear(in_features=8, out_features=64)\n",
    "            self.dropout1 = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.dropout2 = nn.Dropout(p=0.2)\n",
    "            self.fc3 = nn.Linear(32, 64)\n",
    "            self.dropout3 = nn.Dropout(p=0.5)\n",
    "            self.fc4 = nn.Linear(32, 64)\n",
    "            self.dropout4 = nn.Dropout(p=0.7)\n",
    "            self.fc5 = nn.Linear(32, 64)\n",
    "            self.dropout5 = nn.Dropout(p=0.2)\n",
    "            self.fc6 = nn.Linear(32, 64)\n",
    "            self.dropout6 = nn.Dropout(p=0.5)\n",
    "            self.fc7 = nn.Linear(32, 64)\n",
    "            self.dropout7 = nn.Dropout(p=0.7)\n",
    "            self.fc8 = nn.Linear(32, 16)\n",
    "            self.dropout8 = nn.Dropout(p=0.2)\n",
    "            self.fc9 = nn.Linear(16, 8)\n",
    "            self.dropout9 = nn.Dropout(p=0.5)\n",
    "            self.fc10 = nn.Linear(8, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # print(f\"FC1{x.shape}\")\n",
    "            x = self.dropout1(x)\n",
    "            # print(f\"DO1{x.shape}\")\n",
    "            x = F.relu(self.fc2(x))\n",
    "            # print(f\"FC2{x.shape}\")\n",
    "            x = self.dropout2(x)\n",
    "            # print(f\"DO2{x.shape}\")\n",
    "            x = F.glu(self.fc3(x))\n",
    "            # print(f\"FC3 {x.shape}\")\n",
    "            x = self.dropout3(x)\n",
    "            # print(f\"DO3 {x.shape}\")\n",
    "            x = F.glu(self.fc4(x))\n",
    "            # print(f\"FC4 {x.shape}\")\n",
    "            x = self.dropout4(x)\n",
    "            # print(f\"DO4 {x.shape}\")\n",
    "            x = F.glu(self.fc5(x))\n",
    "            # print(f\"FC5 {x.shape}\")\n",
    "            x = self.dropout5(x)\n",
    "            # print(f\"DO5 {x.shape}\")\n",
    "            x = F.glu(self.fc6(x))\n",
    "            # print(f\"FC6 {x.shape}\")\n",
    "            x = self.dropout6(x)\n",
    "            # print(f\"DO6 {x.shape}\")\n",
    "            x = F.glu(self.fc7(x))\n",
    "            # print(f\"FC7 {x.shape}\")\n",
    "            x = self.dropout7(x)\n",
    "            # print(f\"DO7 {x.shape}\")\n",
    "            x = F.relu(self.fc8(x))\n",
    "            # print(f\"FC8 {x.shape}\")\n",
    "            x = self.dropout8(x)\n",
    "            # print(f\"DO8 {x.shape}\")\n",
    "            x = F.relu(self.fc9(x))\n",
    "            # print(f\"FC9 {x.shape}\")\n",
    "            x = self.dropout9(x)\n",
    "            # print(f\"DO9 {x.shape}\")\n",
    "            x = torch.sigmoid(self.fc10(x))\n",
    "            return x\n",
    "\n",
    "    model = BinaryClassifierDropoutV2()\n",
    "    train_dataloader, test_dataloader, validation_dataloader = prepare_data(games_df)\n",
    "    class_weights = [0.75, 1.0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights[1]]))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", cooldown=5, patience=2\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        EarlyStopping(50, 0.001),\n",
    "        num_epochs=200,\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a rough outline of the final recommender system that I will be building in this notebook. It takes in the schedule for upcoming games, and outputs the recommended games that will be the closest and most exciting to watch based on the historical data from NBA games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Load the trained model\n",
    "# model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "# model.load('nba_game_recommender.pkl')\n",
    "\n",
    "# # Load the schedule for the upcoming night's games\n",
    "# schedule = pd.read_csv('nba_schedule.csv')\n",
    "\n",
    "# # Extract relevant features for each game\n",
    "# features = []\n",
    "# for index, row in schedule.iterrows():\n",
    "#     game_id = row['Game_ID']\n",
    "#     home_team = row['Home_Team']\n",
    "#     away_team = row['Away_Team']\n",
    "#     # Extract features from historical data\n",
    "#     feature_vector = get_features(game_id, home_team, away_team)\n",
    "#     features.append(feature_vector)\n",
    "\n",
    "# # Use the model to predict the most exciting game(s) to watch\n",
    "# distances, indices = model.kneighbors(features)\n",
    "# recommended_games = []\n",
    "# for i, dist in enumerate(distances):\n",
    "#     if dist < 0.5:  # Arbitrarily set a threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
